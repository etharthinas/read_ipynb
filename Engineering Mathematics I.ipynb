{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63840d78",
   "metadata": {},
   "source": [
    "## Engineering Mathematics I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff599a",
   "metadata": {},
   "source": [
    "### I. Introduction to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f233155",
   "metadata": {},
   "source": [
    "#### I.1 Scalars and Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e57753",
   "metadata": {},
   "source": [
    "Numbers: Real Numbers ($\\mathbb{R}$) <br>\n",
    "    Natural Numbers ($\\mathbb{N}$) <br>\n",
    "    Integers ($\\mathbb{Z}$) <br>\n",
    "    Complex Numbers ($\\mathbb{C}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac346cb",
   "metadata": {},
   "source": [
    "##### Fields: \n",
    "a field consists of a set of objects ($\\mathbb{F}$), addition ($+$), and multiplication ($\\bullet$). <br>\n",
    "It has five properties: <br>\n",
    "1. It is closed to addition and closed to multiplication\n",
    "2. Addition and multiplication is commutative and associative\n",
    "3. It has a inverse and identity to addition\n",
    "4. It has a inverse and identity to multiplication\n",
    "5. Multiplication can be distributed over addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d478c46",
   "metadata": {},
   "source": [
    "$\\rightarrow$ an element of such a field $\\mathbb{F}$ is called a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf4021",
   "metadata": {},
   "source": [
    "#### I.2 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41668b6a",
   "metadata": {},
   "source": [
    "An n-vector: a list of n scalars over a field (mostly $\\mathbb{R}$ or $\\mathbb{C}$) <br>\n",
    "It is written as this: $x = \\begin{bmatrix} x_1\\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ &emsp;\n",
    "(n is the <em> dimension </em> or <em> size </em> of x) <br>\n",
    "It is different from the <em> transpose </em> of x, which is $x^T = [x_1, x_2, ... x_n]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda0107",
   "metadata": {},
   "source": [
    "$\\mathbb{R}^n$: set of all n-vectors over $\\mathbb{R}$ <br>\n",
    "##### Some interesting vectors: <br>\n",
    "* $0_n$: the zero-vector. Also written as 0. An n-vector whose every entry is 0.\n",
    "* $1_n$: the sum-vector. Also written as 1. An n-vector whose every entry is 1.\n",
    "* $e_i$: the unit vector. An n vector whose every entry is 0, except the $i^{th}$, which is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bfd60",
   "metadata": {},
   "source": [
    "##### Applications of Vectors: <br>\n",
    "- Location or a displacement.\n",
    "- Bag-of-words representation of a document, or word-2-vec representation\n",
    "- Time-series: the evolution in discrete time of some quantity\n",
    "- Image: greyscale or color image\n",
    "- Features: attributes of an entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07b766",
   "metadata": {},
   "source": [
    "Gradient ($\\nabla$) of a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$: <br>\n",
    "Diffrentiate the function with respect to each variable to get the <strong> gradient vector </strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c76c14",
   "metadata": {},
   "source": [
    "#### I.3 Linear Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0b669",
   "metadata": {},
   "source": [
    "##### Addition and Subtraction: <br>\n",
    "For each element add/subtract the corresponding element. <br>\n",
    "$x + y = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2 \\\\ \\vdots \\\\ x_n + y_n \\end{bmatrix}$ <br> <br>\n",
    "Geometric Interpretation: adding displacements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ecfe7",
   "metadata": {},
   "source": [
    "##### Scalar Multiplication: <br>\n",
    "For each element multiply the scalar. <br>\n",
    "$\\alpha x = \\begin{bmatrix} \\alpha x_1\\\\ \\alpha x_2 \\\\ \\vdots \\\\ \\alpha x_n \\end{bmatrix}$ <br> <br>\n",
    "Geometric Interpretation: scaling the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6306bbd",
   "metadata": {},
   "source": [
    "##### Linear Combination: <br>\n",
    "for m different vectors $x^i \\in \\mathbb{R}^n, i = 1, 2, ... m, x$ is a <strong> linear combination </strong> <br> of those vectors with the coefficients $\\alpha_i \\in \\mathbb{R}$ when x is formed thus: <br>\n",
    "$ x = \\alpha_1 x^1 + \\alpha_2 x^2 + ... + \\alpha_m x^m$ <br>\n",
    "\n",
    "Any vector $ b = (b_1, b_2, ... b_n)$ can be represented as a linear combination of the unit vectors: <br> $b = b_1 e_1 + b_2 e_2 + ... b_n e_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7af99",
   "metadata": {},
   "source": [
    "##### Span: <br>\n",
    "with respect to m vectors, span$({x^1, x^2, ... x^n})$ <br> is the set of all linear combinations of those vectors. <br> <br>\n",
    "\n",
    "##### Conic Combination: <br>\n",
    "If all the coefficients of the vectors are non-negative, the span is called the <strong> conic combination. </strong> The conic combination encompasses the region that is between the vectors.<br> <br>\n",
    "\n",
    "##### Affine Combination: <br>\n",
    "If the sum of all the coefficients is 1, the span is called the <strong> affine combination. </strong> The affine combination encompasses the hyperplane that is formed by the vectors. <br> <br>\n",
    "\n",
    "##### Convex Combination: <br>\n",
    "The <strong> convex combination </strong> is both conic and affine. The convex combination encompasses the part of the hyperplane that is between the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35bb20",
   "metadata": {},
   "source": [
    "##### Linear Dependence: <br>\n",
    "A set of vectors $V = (x^i \\in \\mathbb{R}^n)$ is <strong> linearly dependent </strong> if <br>\n",
    "the linear combination of V is 0 while some $\\alpha$ is not 0. <br> \n",
    "If V is linearly dependent, at least one of them can be represented by a linear combination of the others. <br>\n",
    "That is, at least one vector is redundant.\n",
    "<br><br>\n",
    "##### Linear Independence: <br>\n",
    "V is <strong> linearly independent </strong> if it is not linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f5a16",
   "metadata": {},
   "source": [
    "#### I.4 Lengths and Dot Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923e862",
   "metadata": {},
   "source": [
    "##### Inner Product: <br>\n",
    "$x \\bullet y = x^T y = y^T x = x_1 y_1 + x_2 y_2 ... + x_n y_n$ (results in a scalar) <br>\n",
    "\n",
    "Attributes:\n",
    "- $x^T x \\geq 0$ &emsp; $(x^T x = 0 \\iff x = 0)$ <br>\n",
    "- $e_i^T x = x_i$ (ith element)\n",
    "- $1^T x = x_1 + x_2 + ... x_n$ (sum of elements)\n",
    "- $x^T x= x_1^2 + x_2^2 + ... x_n^2$ (sum of squares) <br>\n",
    "- if $w$ is a weight vector and $f$ is a feature vector, then $w^T f$ is total weighted score\n",
    "- $x - (\\frac{1^T x} {n}) 1$: de-meaned vector (transposed that the mean is 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7d90e",
   "metadata": {},
   "source": [
    "##### Linear Functions and Affine Functions: <br>\n",
    "Linear Function: $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "is linear if for any scalars $\\alpha, \\beta$ and n-vectors $x, y$: <br>\n",
    "$f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)$ <br>\n",
    "$\\rightarrow$ every linear function is an inner product of x with an n-vector a. <br>\n",
    "$f(x) = a^T x = a_1 x + a_2 x + ... a_n x$ <br>\n",
    "So, $f(0) = 0$ for every linear function. <br>\n",
    "<br>\n",
    "Affine Function: $f(x) = a^T x + b$ (b is constant) <br>\n",
    "Gradients of Linear/Affine Functions is the vector $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d95603",
   "metadata": {},
   "source": [
    "##### Norm and Distance: <br>\n",
    "The Euclidean Norm: <br>\n",
    "$|x| = \\sqrt{x^T x}$ $\\rightarrow$ induced by the inner product. <br>\n",
    "The Euclidean ($l_2$) Norm is used to measure the magnitude of a vector. <br>\n",
    "$|x - y|$ measures the distance between the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a37ce",
   "metadata": {},
   "source": [
    "##### Properties of Norms: <br>\n",
    "- $|\\alpha|\\Vert x\\rVert = \\Vert\\alpha x\\rVert$\n",
    "- Triangle Inequality: $\\Vert x + y \\rVert \\leq \\Vert x \\rVert + \\Vert y\\rVert$\n",
    "- Nonnegativity: $\\Vert x\\rVert \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b5ad8",
   "metadata": {},
   "source": [
    "##### Other Kinds of Norms: <br>\n",
    "- $l_n$ norm: $(\\sum_{i=1}^n{x_i}^p)^{1/p}$\n",
    "- $l_0$ norm: the number of elements that is not 0. (technically not a norm)\n",
    "- $l_1$ norm: $\\sum_{i=1}^n{|x_i|}$\n",
    "-$l_\\infty$ norm: $\\max(|x_1|, |x_2|, ... |x_n|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff426c",
   "metadata": {},
   "source": [
    "##### Application of Norms: <br>\n",
    "- $rms(x) = \\frac {\\Vert x \\rVert}{\\sqrt n}$\n",
    "- $std(x) = rms(\\hat x)$ &emsp; where, $\\hat x$ is the de-meaned vector.\n",
    "- $rms(x)^2 = avg(x)^2 + std(x)^2$\n",
    "- Chebyshev Inequality: the fraction of elements with <br> \n",
    "$|x_i| \\geq a$ is less than $(\\frac {rms(x)}  {a})^2$\n",
    "- Chebyshev Inequality for De-meaned Vectors: <br>\n",
    "the fraction of elements of x with \n",
    "$|x_i - avg(x)| \\geq \\alpha std(x)$ is less than $\\frac {1}{\\alpha^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524abc22",
   "metadata": {},
   "source": [
    "##### Angles: <br>\n",
    "- Cauchy-Schwarz Inequality: $|x^T y| \\leq \\Vert x\\rVert \\Vert y \\rVert$\n",
    "- Angle: the <strong> angle </strong> between two vectors is defined thus: <br>\n",
    "    $\\theta = \\arccos (\\frac {x^T y}{\\Vert x \\rVert \\Vert y \\rVert}) $\n",
    "- Cosine Similarity: the similarity between two vectors <br>\n",
    "can be measured using the cosine of the angle between them.\n",
    "- Correlation Coefficient: the Cosine Similarity of De-meaned vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5237cba",
   "metadata": {},
   "source": [
    "#### I.5 Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd354bc6",
   "metadata": {},
   "source": [
    "Matrix: an m x n rectangular array of numbers <br>\n",
    "(m is the row count, n is the column count) <br>\n",
    "\n",
    "$A = \\begin{bmatrix} a_{11} & a_{12} & ... & a_{1n} \\\\ a_{21} & a_{22} & ... & a_{2n} \n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & ... & a_{mn} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae99803",
   "metadata": {},
   "source": [
    "##### Applications of Matrices: <br>\n",
    "* images ($a_{ij}$ is the pixel value of a greyscale image)\n",
    "* contingency table\n",
    "* feature data: ($a_{ij}$ is the value of feature $i$ for data $j$)\n",
    "* node-arc matrix: matrix for directed graphs ($a_{ij}$ is the value for node $i$ and arc $j$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9bcd6e",
   "metadata": {},
   "source": [
    "##### Transpose of a Matrix: <br>\n",
    "The transpose of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is denoted $A^T$ <br>\n",
    "$A^T$ is an $n \\times m$ matrix. $(A^T)_{ij} = a_{ji}$ <br>\n",
    "The <strong> row vectors </strong> and the <strong> column vectors </strong> is interchanged. <br>\n",
    "${AB}^T = B^T A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724827f",
   "metadata": {},
   "source": [
    "##### Special Matrices: <br>\n",
    "* Zero Matrix: all entries are zero. \n",
    "* Identity Matrix: all entries are zero, except the entries on the diagonal, which is 1. <br>\n",
    "The identity matrix is a square matrix. It is also a diagonal matrix.\n",
    "* Triangular Matrix: the lower triangular matrix: zeros above the diagonal. <br>\n",
    "the upper triangular matrix: zeros below the diagonal.\n",
    "* Symmetric Matrix: a square matrix that $A = A^T$, that is, $a_{ij} = a_{ji}$\n",
    "* Diagonal Matrix: a square matrix such that all entries are zero except on the diagonal. <br>\n",
    "It is Symmetric, Upper Triangular, and also Lower Triangular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffbea5",
   "metadata": {},
   "source": [
    "##### Block Representations:\n",
    "Matrices whose entries are matrices. Of course, the dimensions must be compatible.\n",
    "$\\rightarrow$ Row and Column Representation of Matrices: <br>\n",
    "$ A = \\begin{bmatrix} a_{1} & a_{2} & ... & a_{n} \\end{bmatrix}$\n",
    "&emsp; where $a_{i}$ is the column vectors of the matrix. <br>\n",
    "$ A = \\begin{bmatrix} \\hat a_{1}^T\\\\ \\hat a_{2}^T \\\\ \\vdots \\\\ \\hat a_{m}^T \\end{bmatrix}$\n",
    "&emsp; where $\\hat a_{i}^T$ is the row vectors of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa19660",
   "metadata": {},
   "source": [
    "##### Matrix Norm: <br>\n",
    "The Frobenius Norm is defined thus: <br>\n",
    "$\\Vert A \\rVert _F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}$ <br>\n",
    "The Distance between two matrices is defined as $\\Vert A-B \\rVert _F$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5eeb0",
   "metadata": {},
   "source": [
    "##### Matrix Operations: <br>\n",
    "Addition, Subtraction, and Scalar multiplication: element-wise. <br>\n",
    "Multiplication with Vectors:\n",
    "$Ax = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & ... & a_{1n} \n",
    "\\\\ a_{21} & a_{22} & ... & a_{2n} \n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "a_{m1} & a_{m2} & ... & a_{mn} \n",
    "\\end{bmatrix}$ \n",
    "$\\begin{bmatrix} x_1\\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ <br>\n",
    "= $\\begin{bmatrix} a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} x_1 +\n",
    "\\begin{bmatrix} a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix} x_2 +\n",
    "...\n",
    "\\begin{bmatrix} a_{1n} \\\\ a_{2n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix} x_n$ <br>\n",
    "= $a_1 x_1 + a_2 x_2 + ... a_n x_n$ &emsp; \n",
    "($a_i$ are column vectors, $x_i$ are components of x.) \n",
    "<br><br>\n",
    "Column Interpretation: <br>\n",
    "A linear combination of the column vectors of A where the coefficients are $x_i$\n",
    "<br> Row Interpretation: <br>\n",
    "A batch inner product of the row-vectors with $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7803411",
   "metadata": {},
   "source": [
    "##### Special Products: <br>\n",
    "* $0x$: the 0 vector.\n",
    "* $Ix$: the original x vector.\n",
    "* De-meaned vector: multiply by A when A is \n",
    "$\\begin{bmatrix} \n",
    "1- \\frac {1}{n} & -\\frac {1}{n} & ... & -\\frac {1}{n} \n",
    "\\\\ -\\frac {1}{n} &  1- \\frac {1}{n} & ... & -\\frac {1}{n} \n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "-\\frac {1}{n} & -\\frac {1}{n} & ... & 1- \\frac {1}{n} \n",
    "\\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d0c26",
   "metadata": {},
   "source": [
    "##### Matrix Products: <br>\n",
    "To multiply matrices, the inner dimensions must agree. <br>\n",
    "$\\rightarrow A \\in \\mathbb{R}^{m \\times p} \\times B \\in \\mathbb{R}^{p \\times n}\n",
    "= C \\in \\mathbb{R}^{m \\times n}$ <br>\n",
    "$C_{ij} = \\hat a_i^T b_j$ <br><br>\n",
    "\n",
    "Array of Matrix-Vector Products:\n",
    "$C = [A b_1 A b_2 ... A b_n]$ <br>\n",
    "Sum of outer products: \n",
    "$C = \\hat a_1^T b_1 + \\hat a_2 ^T b_2 +... \\hat a_p^T b_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca5bad",
   "metadata": {},
   "source": [
    "##### Gram Matrix: <br>\n",
    "The <strong> Gram Matrix </strong> of matrix A \n",
    "$\\in \\mathbb{R}^{m \\times n}$ is defined thus: <br>\n",
    "$G \\in \\mathbb{R}^{n \\times n} = A^T A$ <br>\n",
    "If the Gram Matrix is $I$, then $A$ is a orthogonal matrix. $A$ has to be a square. <br>\n",
    "Orthogonal Matrix: all the row and column vectors are orthogonal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ead57e",
   "metadata": {},
   "source": [
    "##### Powers of Matrices: <br>\n",
    "$A^n = AA ... A$ <br>\n",
    "Negative powers: defined by inverses. (might not even exists) <br>\n",
    "Fractional powers: if A is positive definite, $\\sqrt A$ is defined. <br>\n",
    "In Undirected Edge-Adjacency Matrices: <br>\n",
    "$(A^l){ij}$: the number of ways to get from node i to node j, where one moves l times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf131160",
   "metadata": {},
   "source": [
    "##### Inverses: <br>\n",
    "If A is a square matrix, and a square matrix B satisfies <br>\n",
    "$AB = BA = I$, then B is the inverse of A (denoted $A^-1$), <br>\n",
    "and A is called invertible or non-singular. <br>\n",
    "Not all matrices have inverses (singular) <br><br>\n",
    "\n",
    "In a 2 x 2 case:\n",
    "$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} ^{-1} \n",
    "= \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fa89a",
   "metadata": {},
   "source": [
    "If $A$ and $B$ are invertible, then $AB$ is invertible. <br>\n",
    "If A is invertible, $A^{-1}$ is invertible. <br>\n",
    "If $BA = I$ and $CA = I$, then $B = C$. <br>\n",
    "In square matrices, inverses are unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d9f65",
   "metadata": {},
   "source": [
    "### II. Systems of Linear Equations and LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b465e0",
   "metadata": {},
   "source": [
    "#### II.1 Systems of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e770a",
   "metadata": {},
   "source": [
    "m linear equations of n unknowns: <br>\n",
    "$a_{11}x_1 + a_{12}x_2 + ... a_{1n}x_n = b_1$ <br>\n",
    "$a_{21}x_1 + a_{21}x_2 + ... a_{2n}x_n = b_2$ <br>\n",
    "$\\vdots$ <br>\n",
    "$a_{m1}x_1 + a_{m2}x_2 + ... a_{mn}x_n = b_n$ <br>\n",
    "represented as $Ax = b$, where $A \\in \\mathbb{R}^{m \\times n},\n",
    "x \\in \\mathbb{R}^n, b \\in \\mathbb{R}^m$ <br>\n",
    "$a_{ij}$ is the gain factor from the $j^{th}$ input to the $i^{th}$ output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86e035",
   "metadata": {},
   "source": [
    "##### Linear Functions <br>\n",
    "$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is linear if for all \n",
    "$x, y, \\alpha, \\beta$: <br>\n",
    "$f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)$ <br>\n",
    "$\\rightarrow$ All linear functions are represented by a matrix. <br>\n",
    "All functions can be approximated as a Linear Function using the Jacobian. <br>\n",
    "When $x \\approx x_0$, $f(x) \\approx f(x_0) + D_f(x_0)(x - x_0)$ &emsp; where $D_f$ is the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a500e2e8",
   "metadata": {},
   "source": [
    "##### Applications: <br>\n",
    "* Function fittiing $\\rightarrow$ Vandermonde matrix\n",
    "* Regression\n",
    "* Forecasting\n",
    "* Control \n",
    "* Markov Matrix: a matrix that each columns's sum is 1. <br> \n",
    "It presents states and transitions as probabilities. <br>\n",
    "ex. $\\begin{bmatrix} 0.9 & 0.2 \\\\ 0.1 & 0.8 \\end{bmatrix}$ <br>\n",
    "$\\rightarrow$ 0.9 is the probability that state 1 will remain state 1, <br> 0.1 is the probability state 1 will change to state 2, <br> 0.8 is the probability state 2 will remain state 2, <br> and 0.2 is the probability state 2 will change to state 1. <br>\n",
    "$\\rightarrow M^N$ is the states after the $N^{th}$ iteration. <br>\n",
    "Also the equilibrium can be found by solving $Mx = x$, or the eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff2e2",
   "metadata": {},
   "source": [
    "#### II.2 Geometric Views on Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b39e8",
   "metadata": {},
   "source": [
    "##### Hyperplanes: <br>\n",
    "\n",
    "$H = \\{x \\in \\mathbb{R}^n : a^Tx = b\\}$ <br>\n",
    "If $a = 0$, the solution is non-existent or infinite. <br>\n",
    "$H_0 = \\{x \\in \\mathbb{R}^n : a^Tx = 0\\}$ <br>\n",
    "$\\rightarrow H_0$ is the set of vectors that is orthogonal to $a$. <br> \n",
    "If $v \\in H_0$, for a particular element $x_0$ of H, for any $x \\in H, x = v + x_0$ <br>\n",
    "$\\rightarrow$ $H = \\{x \\in \\mathbb{R}^n: x = x_0 + tv, t \\in \\mathbb{R}, v \\in H_0 \\}$ <br>\n",
    "<br> Halfspaces: $H_1 = \\{x \\in \\mathbb{R}^n : a^Tx \\geq b\\}$ <br>\n",
    "$H_2 = \\{x \\in \\mathbb{R}^n : a^Tx \\leq b\\}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f91258",
   "metadata": {},
   "source": [
    "##### Row Space and Column Space:<br>\n",
    "Row Picture: intersection of $m$ hyperplanes in $n$-dimensions. <br>\n",
    "Column Picture: linear combination of $n$ $m$-vectors to make $b$. <br>\n",
    "Column-Space: $C(A) = span(a_1, a_2, ... a_n)$ <br>\n",
    "The Column Space is the Range of $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc0d3a",
   "metadata": {},
   "source": [
    "##### Onto (Surjective) and One-to-one (Injective) Functiions: <br>\n",
    "Surjective Functions:<br>\n",
    "every element in the codomain has at least one corresponding element in the domain. <br><br>\n",
    "Injective Functions:<br>\n",
    "every element in the range has only one corresponding element in the domain. <br><br>\n",
    "Bijective Functinos: <br>\n",
    "the function is both surjective and injective $\\rightarrow$ the function is invertible. (Non-singualar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6cee6",
   "metadata": {},
   "source": [
    "##### Nullspace (Kernel): <br>\n",
    "$ N(A) = \\{x = \\mathbb{R}^n : Ax = 0\\}$ <br>\n",
    "If there is an $x \\in N(A)$ that $x \\neq 0$, then $f(x) = Ax$ is not injective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c975ef8",
   "metadata": {},
   "source": [
    "#### II.3 Solutions to Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fdfcb",
   "metadata": {},
   "source": [
    "##### Equivalent Linear Systems: <br>\n",
    "Elementary Row Operations:\n",
    "* Multiplication of one row by a nonzero scalar $\\alpha$.\n",
    "* Replacement of (row $k$) by (row $k$) + $\\alpha$(row $l$), \n",
    "where $k \\neq l$ and $\\alpha$ is a scalar\n",
    "* Interchanging two rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e973ad",
   "metadata": {},
   "source": [
    "##### Gaussian Elimination: <br>\n",
    "Input: $Ax = b$ <br>\n",
    "Goal: Use the elementary row operations to find an equivalent system $Ux = c$ that U is of echelon form. <br>\n",
    "$\\rightarrow$ make zeros below the diagonal to eliminate variables. <br>\n",
    "The coefficients on the diagonal are called <strong> pivots </strong>. <br>\n",
    "Sometimes a matrix - vector product does not have a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57c2e",
   "metadata": {},
   "source": [
    "$Ax = b \\iff$ every $y \\in \\mathbb{R}^m$, if $y^TA = 0, y^Tb = 0$ <br>\n",
    "\n",
    "Calculating Time: approx. $\\frac{1}{3} n^3$ <br>\n",
    "\n",
    "RREF: (Row-Reduced-Echelon Form) <br>\n",
    "All rows have only pivots in them. Calculated from the Gauss-eliminated U, <br>\n",
    "and back-substituted using the Jordan elimination. <br>\n",
    "$\\rightarrow$ $R = I \\iff$ A is invertible. $\\iff N(A) = {0}$ &emsp; (N(A) is the nullspace of A)\n",
    "<br> $\\iff$ the columns of $A$ are independent $\\iff$ the rank of $A$ is $n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d8a17",
   "metadata": {},
   "source": [
    "##### LDU Decomposition: <br>\n",
    "\n",
    "$L$: Inverse of the elimination matrices used for GE. <br>\n",
    "$D$: Diagonal matrix consisting of pivots. <br>\n",
    "$U$: the matrix that is the echelon form reduced by GE, its pivots being all 1. <br> <br>\n",
    "\n",
    "If row exchange is needed, then include Permutation Matrix P. <br>\n",
    "$P$: A matrix that is a row exchange of the Identity Matrix. <br> <br>\n",
    "\n",
    "$\\rightarrow$ $PA = LDU$ &emsp; $\\rightarrow$ always possible! <br>\n",
    "If A is symmetric, $\\rightarrow$ $PA = LDL^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd5a0e",
   "metadata": {},
   "source": [
    "##### Least Squares: <br>\n",
    "If $Ax = b$ does not have a solution, one can find x such that $Ax$ is close to b. <br>\n",
    "$\\rightarrow$ find x that minimizes $\\Vert Ax - b \\rVert$ <br>\n",
    "$A^TAx = A^Tb$ always has a solution, and it is called <strong> normal equation. </strong><br>\n",
    "Its solution solves least square problems. <br>\n",
    "If the columns of A are independent, $(A^TA)^-1$ exists, making $x = (A^TA)^{-1}(A^T)b$. <br>\n",
    "$\\rightarrow$ used in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c06843",
   "metadata": {},
   "source": [
    "##### Non-negative, Integer Solutions: <br>\n",
    "Some equations $Ax = b$ require nonnegative variables or integer variables. <br>\n",
    "Nonnegative systems of equations become systems of inequalities: <br>\n",
    "$Ax = b, x \\geq 0$ $\\rightarrow$ Linear Programming <br>\n",
    "Integer systems are called Diophantine Equations: <br>\n",
    "$Ax = b, x \\in \\mathbb{Z}$ $\\rightarrow$ Diophantine Equations <br>\n",
    "When nonnegative and integer systems merge to one, it becomes Integer Programming. <br>\n",
    "$Ax = b, x \\geq 0, x \\in \\mathbb{Z}$ $\\rightarrow$ Integer Programming <br>\n",
    "Integer Programming is not yet solvable. (P-NP problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc604b0",
   "metadata": {},
   "source": [
    "### III. Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140daec",
   "metadata": {},
   "source": [
    "#### III.1 Definition of Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f3d1d",
   "metadata": {},
   "source": [
    "##### Vector Space: <br>\n",
    "* set V of elements called vectors.\n",
    "* a sum operation which returns a vector in V.\n",
    "* a scalar multiplication operation which returns a vector in V\n",
    "* a element 0 called the zero vector.\n",
    "<br><br>\n",
    "\n",
    "##### Properties: <br>\n",
    "* $x + y = y+x$\n",
    "* $0 + x = x$, $1x = x$\n",
    "* $a(x+y) = ax + ay, (a+b)x = ax + bx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34808664",
   "metadata": {},
   "source": [
    "##### Subspaces: <br>\n",
    "A subset of the vector space V which by itself is a subspace. <br>\n",
    "$\\rightarrow$ Closed under linear combination. <br>\n",
    "$\\rightarrow$ Must contain the 0 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef3461",
   "metadata": {},
   "source": [
    "##### Four Fundamental Subspaces: <br>\n",
    "1.Column Space of A $\\in \\mathbb{R}^{m \\times n}: C(A)$ <br>\n",
    "span($a_1, a_2,... a_n) \\in \\mathbb{R}^m$ <br>\n",
    "2. Null Space (Kernel) of $A: N(A)$ <br>\n",
    "$\\{x \\in \\mathbb{R}^n | Ax = 0\\}$ $\\rightarrow$ always contains the $0$ vector. <br>\n",
    "3. Row Space of $A: C(A^T)$ <br>\n",
    "span ($\\tilde a_1 ^T, \\tilde a_2 ^T, ... \\tilde a_m ^ T) \\in \\mathbb{R}^n$ <br>\n",
    "4. Left Null Space of $A: N(A^T)$ <br>\n",
    "$\\{x \\in \\mathbb{R}^m | x^TA = 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8a978",
   "metadata": {},
   "source": [
    "##### Basis, Dimension, Rank: <br>\n",
    "* Basis: if $V = span(v_1, v_2, ... v_k)$ and $\\{v_1, v_2, ... v_k\\}$ is independent, <br>\n",
    "$\\{v_1, v_2, ... v_k\\}$ is the basis of $V$. $\\rightarrow$ bases are not unique, but the number of them is the same.\n",
    "* Dimension: the number of vectors in a basis. It is unique for any subspace $V$.\n",
    "* Rank: the maximal number of independent vectors row or column vectors of a matrix $A$. <br>\n",
    "The Pivots of $rref(A)$ is the only useful vectors in A. <br>\n",
    "Rank($A$) = dim($C(A)$) = dim($C(A^T)$) $\\rightarrow$ maximally max($m,n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01b309",
   "metadata": {},
   "source": [
    "If a solution to the equation $Ax = b$ exists $\\iff b \\in C(A) \\iff b \\in N(B)$ for some $B$. <br>\n",
    "In elimination: the row space is preserved, but not the column space. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960529b",
   "metadata": {},
   "source": [
    "$A = \\begin{bmatrix} 1 & 1 & 2 \\\\ 2 & 1 & 3 \\\\ 3 & 1 & 4 \\\\ 4 & 1 & 5 \\end{bmatrix}$ &emsp;\n",
    "$U = \\begin{bmatrix} 1 & 1 & 2 \\\\ 0 & -1 & -1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ &emsp;\n",
    "$R = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ <br> <br>\n",
    "\n",
    "1) $C(A) = span(\\{a_1, a_2\\})$ <br>\n",
    "Since the pivots are in column 1 and column 2 in $R$, C(A) is made out of $a_1, a_2$ in $A$. <br>\n",
    "$\\rightarrow$ $dim(C(A)) =$ # of pivots ($rank(A)$) $= 2$ <br>\n",
    "\n",
    "2)$N(A) = \\alpha \\begin{bmatrix} -1 & 1 & 1 \\end{bmatrix}$. <br>\n",
    "Since A is not full-rank, $N(A)$ is not trivial. <br>\n",
    "The special solution consists of the third (non-basic) column of $R$ times -1, and then a 1. <br>\n",
    "The reason why is because $a_3$ is non-basic, and can be formed out of the first two columns, <br>\n",
    "the coefficients being $r_3$. <br>\n",
    "Therefore, $dim(N(A)) = n - rank(A)$ \n",
    "\n",
    "3) $C(A^T) = C(U^T) = C(R^T)$ <br>\n",
    "In elimination, the row-space is preserved. Therfore $C(A^T) = C(U^T) = C(R^T)$. <br>\n",
    "The basis of $C(A^T)$ is the first two rows of $R$, that is, the rows that contain pivots. <br>\n",
    "That is, $dim(C(A^T)) = dim(C(A)) = rank(A)$ $\\rightarrow$ Fundamental Theorem of Linear Algebra. <br>\n",
    "\n",
    "4) $N(A^T)$ <br>\n",
    "The rows that do not contain the pivots are eliminated to 0. <br>\n",
    "The elimination matrix's rows which eliminate to 0 form the left-nullspace of A. <br>\n",
    "$dim(N(A^T)) = m - rank(A)$ <br>\n",
    "\n",
    "$C(A^T)$ and $N(A)$ is orthogonal in $\\mathbb{R}^n$. <br>\n",
    "$C(A)$ and $N(A^T)$ is orthogonal in $\\mathbb{R}^m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1e7ec7",
   "metadata": {},
   "source": [
    "#### III.2 Complete Solutions and the Null Space "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe6d0",
   "metadata": {},
   "source": [
    "To get a complete solution to $Ax = b$: <br>\n",
    "* Find a particular solution (Gaussian Elimination)\n",
    "* Add to it linear combinations of the bases of $N(A)$.\n",
    "* There is $n - rank(A)$ non-basic columns in rref(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907771b",
   "metadata": {},
   "source": [
    "One can order R such that <br>\n",
    "$R' = \\begin{bmatrix} I_{r \\times r} & F_{r \\times n-r} \\\\ 0_{m-r \\times r} & 0_{m-r \\times n-r} \\end{bmatrix}$ <br>\n",
    "The special solutions, or the bases of the NullSpace is $\\begin{bmatrix} -F_{r \\times n-r} \\\\ I_{n-r \\times n-r} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dd4a2",
   "metadata": {},
   "source": [
    "#### III.3 Orthogonality of the Four Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992230d",
   "metadata": {},
   "source": [
    "Two subspaces are orthogonal if for every vector in the subspaces are orthogonal to each other. <br>\n",
    "$\\rightarrow$ the only intersection is the zero vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c85b6a",
   "metadata": {},
   "source": [
    "The ColumnSpace and the Left NullSpace are Orthogonal. <br>\n",
    "pf) if $Ax \\ \\text{in the Column Space} = y \\ \\text{in the left Null Space}$ <br>\n",
    "$A^TAx = A^Ty = 0$ <br>\n",
    "$x^T A^T Ax = 0 \\rightarrow {(Ax)}^T Ax = 0 \\rightarrow \\Vert Ax \\rVert = 0 \\rightarrow Ax = 0$ <br> <br>\n",
    "Also, the RowSpace and the NullSpace are Orthogonal. <br>\n",
    "pf) the same for $A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598db713",
   "metadata": {},
   "source": [
    "#### III.4 More on Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e70bac3",
   "metadata": {},
   "source": [
    "* $f''(x) + f(x) = 0$ <br>\n",
    "$f(x) = a\\cos(x) + b\\sin(x)$ <br>\n",
    "basis: $\\cos(x), \\sin(x)$\n",
    "* $f(x) = a + bx + cx^2$ <br>\n",
    "basis: $\\{1, x, x^2\\}$ <br>\n",
    "*$V + W$ when $V$, $W$ are vector spaces (Minkowski sum) <br>\n",
    "$V + W = \\{x + y | x \\in V, y \\in W\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d1db9",
   "metadata": {},
   "source": [
    "##### More on Independence, Bases, and Dimensions <br>\n",
    "$A_{n \\times k} = [v_1, v_2, ... v_k]$ <br>\n",
    "* if $v_i$ are independent: $k \\leq n$\n",
    "* if $v_i$ are independent: $rank(A) = k, N(A) = {0}$\n",
    "* if $v_i$ are a basis of $V$: $dim(V) = k$ <br>\n",
    "* two bases $v_i$ and $w_i$ are basis of $V$: $len(v_i) = len(w_i)$\n",
    "pf) WLOG $len(w_i) > len(v_i)$. <br>\n",
    "$\\rightarrow$ one can construct a matrix C that <br>\n",
    "$\\begin{bmatrix}w_1 & w_2 & ... & w_p\\end{bmatrix} = \\begin{bmatrix}v_1 & v_2 & ... & v_k\\end{bmatrix} \n",
    "\\begin{bmatrix} c_1 & c_2 & ... & c_p \\end{bmatrix}$ <br>\n",
    "$N(C) \\subset N(W)$, since if $Cx = 0 \\rightarrow Wx = VCx = 0$ <br>\n",
    "but since $k < p$, $N(C)$ contains a non-trivial solution. <br>\n",
    "$\\rightarrow$ $W$ is dependent.\n",
    "* if $dim(V) = k$, any independent set $S$ with $len(S) = k$ form a basis of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3656855",
   "metadata": {},
   "source": [
    "* $rank(A) = dim(C(A)) = dim(C(A^T)) = $ # of pivots$ = r$\n",
    "* $r \\leq min(m, n)$\n",
    "* $r = m < n\\rightarrow$ full row rank. $\\rightarrow$ has $\\infty$ solutions ($N(A^T) = \\{0\\}$) <br>\n",
    "Independent Rows. Right-Inverse exists. $R = \\begin{bmatrix} I & F\\end{bmatrix}$\n",
    "* $r = n < m\\rightarrow$ full column rank. $\\rightarrow$ has $1$ or  $0$ solutions $N(A) = \\{0\\}$. <br>\n",
    "Independent Columns. Left-Inverse exists. $R = \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}$\n",
    "* $r = m = n \\rightarrow$ has exactly $1$ solution $\\rightarrow$ invertible, non-singular. <br>\n",
    "$R = \\begin{bmatrix} I \\end{bmatrix}$\n",
    "* $r < m, r < n \\rightarrow$ has $0$ or $\\infty$ solutions <br>\n",
    "$R = \\begin{bmatrix} I & F \\\\ 0 & 0 \\end{bmatrix}$ <br> <br>\n",
    "\n",
    "pf) $N(A) = \\{0\\}$ <br>\n",
    "$\\rightarrow N(A^T A) = \\{0\\}$ <br>\n",
    "$\\rightarrow$ Inverse of $A^T A$ exists <br>\n",
    "$\\rightarrow {A^T A}^{-1} A^T$ is left inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09de6fa",
   "metadata": {},
   "source": [
    "### IV. Graphs and Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3698308",
   "metadata": {},
   "source": [
    "#### IV.1 Graphs and Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b3d36",
   "metadata": {},
   "source": [
    "##### Graphs: <br>\n",
    "Graph: a pair composed of the set of Vertices(Nodes) and the set of Edges(Arcs) <br>\n",
    "$G = (V, E)$ <br>\n",
    "Undirected Graph: all arcs are non-directional. <br>\n",
    "Directed Graph: all arcs are directional. <br>\n",
    "Simple Graph: a graph that does not have Self-loops or repeated edges. <br>\n",
    "Degree of a Node: the number of arcs leading to it + the number of arcs leading out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e878e",
   "metadata": {},
   "source": [
    "##### Networks: <br>\n",
    "Networks: a graph that has numerical properties on edges and nodes. <br>\n",
    "Generally, networks mean simple directed graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393325b2",
   "metadata": {},
   "source": [
    "##### Subgraphs: <br>\n",
    "If $G = (V, E)$ and $G' = (V', E')$ and $V' \\subset V, E' \\subset E$,  <br>\n",
    "and the nodes are linked in the same way, then $G'$ is a subgraph of $G$. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77529b",
   "metadata": {},
   "source": [
    "##### Special Graphs: <br>\n",
    "* Complete Graph: evey node pair has a unique edge between them.\n",
    "* Bipartite Graph: every edge connects between two node sets.\n",
    "* Tree: there is only one route between two nodes.\n",
    "* Star: one inner node and $k$ leaf nodes. \n",
    "* Forest: a union of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4c799",
   "metadata": {},
   "source": [
    "#### IV.2 Routes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f6267",
   "metadata": {},
   "source": [
    "* Walk: an ordered sequence of nodes and arcs.\n",
    "* Trail: a walk with distinct edges.\n",
    "* Path: a trail with distinct nodes. Closed path $\\rightarrow$ cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466dc907",
   "metadata": {},
   "source": [
    "##### Matrix Form of Undirected Graphs: <br>\n",
    "Node-node adjacency matrix. An $n \\times n$ matrix where there are $n$ nodes. <br>\n",
    "$a_{ij} = 1$ if there is a connection between node $i$ and node $j$. If not, it is 0. <br>\n",
    "Symmetric matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67262ed5",
   "metadata": {},
   "source": [
    "##### Matrix Form of Directed Graphs: <br>\n",
    "Node-arc incidence matrix. An $n \\times m$ matrix where there are $n$ nodes and $m$ arcs. <br>\n",
    "$a_{ij} = 1$ if arc $j$ leads to node $i$. -1 if arc $j$ goes out from node $i$. If not, 0. <br>\n",
    "Rank: $n-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d473fa9",
   "metadata": {},
   "source": [
    "### V. Orthogonality and QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015898be",
   "metadata": {},
   "source": [
    "#### V.1 Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a903b",
   "metadata": {},
   "source": [
    "Orthogonality: two non-zero vectors are orthogonal, then $v^T w = 0$ <br>\n",
    "Orthogonal Vectors: $\\{v_1, v_2, ... v_n\\}$ is orthogonal $\\iff$ $v_i \\perp v_j = 0 \\ (i \\neq j)$ <br>\n",
    "Orthonormal Vectors: $\\{v_1, v_2, ... v_n\\}$ is orthonormal $\\iff$ $\\{v_1, v_2, ... v_n\\}$ is orthogonal and $\\Vert v_i \\rVert = 1$ <br>\n",
    "$\\rightarrow$ Non-zero orthogonal vectors are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd964654",
   "metadata": {},
   "source": [
    "##### Orthogonal Matrix: <br>\n",
    "$A = \\begin{bmatrix} v_1 & v_2 & ... & v_n \\end{bmatrix}$ when $\\{v_1, v_2, ... v_n\\}$ are orthonormal. <br> Then $A$ has an inverse. $A^{-1} = A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861eb7e",
   "metadata": {},
   "source": [
    "##### Bases and Orthgonality: <br>\n",
    "Any set of independent bases can form coordinates. <br>\n",
    "If $x$ is the coordinate under the current system, <br>\n",
    "$A^{-1}x$ is the new coordinate under the new system, <br>\n",
    "since $A A^{-1}x = x.$ <br>\n",
    "If $A$ is an orthogonal matrix, then it is efficient to find $A^{-1}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1f721",
   "metadata": {},
   "source": [
    "##### Orthogonality of Subspaces: <br>\n",
    "Subspace $V$ and $W$ are orthogonal ($S \\perp T$) \n",
    "$\\iff$ $s \\perp t$ for any $s \\in S, t \\in T$ <br>\n",
    "If $S \\perp T$, then $S \\cap T = \\{0\\}$ <br>\n",
    "$\\rightarrow N(A) \\perp C(A^T), C(A) \\perp N(A^T)$ <br>\n",
    "<br>\n",
    "##### Orthogonal Complements: <br>\n",
    "If every vector that is orthogonal to $S$ is in $T$, then $T$ is the orthogonal complement of $S. (T = S^{\\perp})$ <br>\n",
    "Dimensions must add up in order to be the orthogonal complement. <br>\n",
    "$N(A) = C(A^T)^{\\perp}, C(A) = N(A^T)^{\\perp}$ <br>\n",
    "$\\rightarrow$ for any $x \\in \\mathbb{R}^n$, $x = x_r + x_n$ such that $x_r \\in C(A^T), x_n \\in N(A)$. <br> Any subspace can be a nullspace of a matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f54401",
   "metadata": {},
   "source": [
    "#### V.2 Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67270d7c",
   "metadata": {},
   "source": [
    "##### Projection onto a Subspace: <br>\n",
    "Decomposing a given vector into two components, <br>\n",
    "one in the subspace and the other orthogonal to the subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3016fca",
   "metadata": {},
   "source": [
    "##### Projection Matrix: <br>\n",
    "The projection of $b$ onto $C(A)$ is thus: <br>\n",
    "$p = Pb$, where $P = A(A^T A)^{-1} A^T$, where $A$ is full column rank <br>\n",
    "$r = b - p = (I-P)b$ <br>\n",
    "$p \\in C(A), r \\in N(A^T)$ <br><br>\n",
    "In 1-d case: $p = \\frac{aa^T}{a^Ta} b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708fb52",
   "metadata": {},
   "source": [
    "##### Properties of Projection Matrices: <br>\n",
    "Projection Matrices are\n",
    "* Symmetric $(P = P^T)$\n",
    "* Idempotent $(P^2 = P)$ \n",
    "<br>\n",
    "\n",
    "If a matrix $P$ is symmetric and idempotent, it is a projection matrix. <br>\n",
    "pf) In order for $P$ to be a projection matrix, for any vector $b$, <br>\n",
    "$Pb \\perp b - Pb$.\n",
    "$P^T (b - Pb) = P(b-Pb) = Pb - P^2 b = Pb - Pb = 0$ <br>\n",
    "$\\rightarrow Pb \\in C(P), b-Pb \\in N(P^T) \\rightarrow Pb \\perp b-Pb$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c25648",
   "metadata": {},
   "source": [
    "##### V.3 Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8876e59",
   "metadata": {},
   "source": [
    "The projection onto $C(A)$ solves the least square problem: <br>\n",
    " minimize $\\Vert Ax - b\\rVert$, where $x \\in \\mathbb{R}^n$ <br>\n",
    "That is, $\\hat x = A(A^TA)^{-1}A^T x$ is a pseudo-answer to $Ax = b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c84ed64",
   "metadata": {},
   "source": [
    "Least Norm problem: <br>\n",
    "minimize $\\Vert x \\rVert$, where $Ax = b, x \\in \\mathbb{R}^n$ <br>\n",
    "$x = A^T(AA^T)^{-1}b$, since the projection of $x$ onto $N(A) = 0$ <br>\n",
    "The projection of $x$ onto $N(A)$ \n",
    "$\\rightarrow (I - A^T(AA^T)^{-1}A)x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47680b44",
   "metadata": {},
   "source": [
    "#### V.4 QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a7a91",
   "metadata": {},
   "source": [
    "Orthonormal Matrix: <br>\n",
    "$A = \\begin{bmatrix} q_1 & q_2 & ... & q_n\\end{bmatrix}$ where $q_i$ is orthogonal to each other. <br>\n",
    "Then, $A^TA = I$, and then $P = AA^T$ <br>\n",
    "Projection onto a Orthogonal Matrix: <br>\n",
    "$p = AA^Tb$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce285cef",
   "metadata": {},
   "source": [
    "Gram-Schimdt Algorithm: <br>\n",
    "Decompose $A$ into orthonormal vectors. <br>\n",
    "(Assume $A$ is full-rank. If not, leave only linearly independent vectors)<br>\n",
    "* $\\hat q_1 = a_1$\n",
    "* $q_1 = \\frac {\\hat q_1}{\\Vert \\hat q_1 \\rVert}$\n",
    "* $\\hat q_2 = a_2 - q_1q_1^Ta_1$\n",
    "* $q_2 = \\frac{\\hat q_2}{\\Vert \\hat q_2 \\rVert}$\n",
    "<br> $\\vdots$\n",
    "* $\\hat q_n = a_n - (q_1q_1^Ta_1 + q_2q_2^Ta_2 + ... q_{n-1}q_{n-1}^Ta_{n-1})$\n",
    "* $q_n = \\frac{\\hat q_n}{\\Vert \\hat q_n \\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7fcef",
   "metadata": {},
   "source": [
    "QR Decomposition: given a matrix $A = \\begin{bmatrix} a_1 & a_2 & ... a_n \\end{bmatrix}$, factor it into form $A = QR$ where <br>\n",
    "$Q = \\begin{bmatrix} q_1 & q_2 & ... & q_n \\end{bmatrix}, \n",
    "R = \\begin{bmatrix} q_1^Ta_1 & q_1^Ta_2 & ... & q_1^Ta_n \\\\ 0 & q_2^Ta_2 & ... & q_2^Ta_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ 0 & 0 & ... & q_n^T a_n \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350af42",
   "metadata": {},
   "source": [
    "### VI. Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5dff9",
   "metadata": {},
   "source": [
    "#### VI.1 Definition of the Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ea27f",
   "metadata": {},
   "source": [
    "Defining Properties of the Determinant: <br>\n",
    "* $\\det(I) = 1$.\n",
    "* sign is changed when rows are exchanged.\n",
    "* row-linear: $\\det(a_1^T, a_2^T, ... a_n^T + t b_n^T) = \\det(a_1^T, a_2^T, ... a_n^T) + t\\det(a_1^T, a_2^T, ... b_n^T)$\n",
    "\n",
    "<br>\n",
    "The function that satisfies all these preperties uniquely exist. <br>\n",
    "(Only defined for square matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14c3e",
   "metadata": {},
   "source": [
    "Properties of the Determinant: <br>\n",
    "* 2 equal rows $\\rightarrow \\det(A) = 0$\n",
    "* elimination $\\rightarrow \\det(A) = \\det(EA)$\n",
    "* rows of zeros $\\rightarrow \\det(A) = 0$\n",
    "* $A$ is triangular $\\rightarrow \\det(A) = d_1d_2...d_n$ where $d_i$ is the diagonal entries.\n",
    "* $\\det(A) = (-1)^kd_1d_2...d_n$, where $k$ is the number of row exchanges.\n",
    "* $A$ is singular $\\rightarrow \\det(A) = 0$\n",
    "* $\\det(A)\\det(B) = \\det(AB)$\n",
    "* $\\det(A) = \\det(A^T), \\det(A^{-1}) = \\frac{1}{\\det(A)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cb060",
   "metadata": {},
   "source": [
    "$\\det(A) = \\sum_{\\sigma\\in\\Omega}(a_{1\\sigma_1}a_{2\\sigma_2}...a_{n\\sigma_n})\\det(P_\\sigma)$ <br>\n",
    "But it is an inefficient way to calculate determinants. <br>\n",
    "$\\rightarrow$ if all the elements in $A$ are integers, then the determinant is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669daf5",
   "metadata": {},
   "source": [
    "#### VI.2 Calculating Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92238771",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
